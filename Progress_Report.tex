
\documentclass[12pt]{article}
\usepackage{fullpage,enumitem,amsmath,amsthm,amsfonts,amssymb,graphicx,float,listings}

\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}

% Shortcuts.
% One can define new commands to shorten frequently used
% constructions. As an example, this defines the R and Z used
% for the real and integer numbers.
%-----------------------------------------------------------------
\def\RR{\mathbb{R}}
\def\ZZ{\mathbb{Z}}

% Similarly, one can define commands that take arguments. In this
% example we define a command for the absolute value.
% -----------------------------------------------------------------
\newcommand{\abs}[1]{\left\vert#1\right\vert}

% Operators
% New operators must defined as such to have them typeset
% correctly. As an example we define the Jacobian:
% -----------------------------------------------------------------
\DeclareMathOperator{\Jac}{Jac}

%-----------------------------------------------------------------
\title{CS 221 Project Progress}
\author{Jon Braatz \& Lance Lamore}


\date{November 2018}

\begin{document}

\maketitle

\abstract{ We propose to create a model for predicting YouTube views for a video
  posted to an uploader's channel based on a candidate video title and statistics from
  previously uploaded videos to the uploader's channel. To start with, we are working with
  a dataset of composed of 6623 random channels from youtube. From this list of channel, we created a simple feature vector for a video based on the channel statistics and began running a simple linear regression on the features in relation to the views of the video.}
  
\section{Goal and Design of the Model}

Our project is a regression task that, when given a candidate YouTube
video title and channel to upload it to, predicts the number of views a video
uploaded to the channel with that title would likely get. The statistics that
will be used to predict the view count will be the video statistics available
through the YouTube API of a subset of videos previously uploaded to the
provided channel. In particular, these statistics are:
\begin{enumerate}
  \item \texttt{ViewCount}
  \item \texttt{CommentCount}
  \item \texttt{LikeCount}
  \item \texttt{DislikeCount}
  \item \texttt{FavoriteCount}
  \end{enumerate}
The subset of videos to choose these statistics from will be the k-nearest
neighbors of the candidate video according to some similarity metric.
We chose two similarity metrics, one based on the video timestamps by considering
the statistics of the 10 most recently uploaded videos, and another based on
cosine similarity of the uploaded video titles to the candidate title.
The feature vectors are the same for both
models, and they are the video statistics of the ``most similar'' videos in the
channel to the candidate video, the model just differs in which videos those are.
We use a gradient boosted tree regressor to turn these statistics into a view
count prediction, and we use the \texttt{xgboost} machine learning library to do
so.

To train the regressor, we needed a dataset of ``candidate'' videos and
channels that they'd be uploaded to. To gather this data, we scanned through a
currently existing dataset with randomly sampled YouTube channel identifiers,
and using the YouTube search API we could pull statistics for all videos in
those channels. For our training dataset, we chose the most recently
uploaded video to each channel to act as a ``candidate'' video for that channel,
and then pulled the statistics of the 10 most similar videos to that one, both
in terms of publishing date and similarity of title according to cosine similarity.
Using the true view count of the most recently uploaded videos along with these
statistics, we could train our gradient boosted regressor.

\section{Algorithms Used}
Our problem is fundamentally a regression problem, and since we don't want to
limit ourselves to linear models, we decided to use gradient boosted regression.
We'll define a couple of terms first.

\begin{enumerate}
\item Regression: Regression consists of predicting a target value (in our case,
  the view count of a candidate video) based on several numerical or categorical
  predictors. In our case, the predictors are video statistics for the 10 videos
  in the channel that are most similar to the candidiate video according to some
  similarity metric.
\item Decision Tree: A decision tree is a method of regression that is built
  top-down from a root node and involves partitioning the data into subsets that
  contain instances with similar values. At each interior node of the tree, we
  apply a test to one of the predictor variables, and depending on the value of
  the outcome of the test we either go to the left or right subtree. Predictions
  of the target variable correspond to leaves in the tree. A decision tree
  regression algorithm will construct a decision tree based on training data
  that minimizes the least square error of the predictions and training target
  values, subject to a regularization term.
\item Gradient Tree Boosting: Gradient tree boosting is based on the idea of
  creating a potentially weak regression tree on the data, creating a regression
  tree for the residuals of the first model, and then sequentially applying this
  idea to create a strong regressor out of weak regressors.
\end{enumerate}

We use gradient boosted regressors from two libraries for our project, one from
\texttt{sklearn} and the other from \texttt{xgboost}. The first library offers
an easy way to visualize the feature importances that are learned by the
regressor, and the second offers better performance when predicting new values.
Also, since our target variable, view count, ranges over many orders of
magnitude, it is better to predict the logarithm of the view count rather than
the view count itself. For this reason, by taking logarithms of the true view
counts in the training data, our loss function was effectively the mean squared
error of the logs.

\section{Preliminary results}

We were able to beat the baseline performance when using both a video similarity
metric based on timestamp and based on cosine similarity of the title. Plotting
the feature importances of the regression showed that the important features
match up with what one would intuitively expect. The best predictor of view
counts are the view counts of the most similar videos (either most recent or
closest in subject matter), and their predictive power drops off as the
similarity to the candidate video goes down. Other statistics besides view
counts were not found to be predictive.

On our dataset, the baseline's mean squared error between the log of the
predicted view count and the log of the true view counts in the training set was
found to be 2.2, and the oracle's was found to be .32. Our regressions based on
the statistics of most recent videos and videos with the highest cosine
similarity to the candidate title were 1.92 and 2.07 respectively, which beats
the baseline but leaves much room for improvement. The $R^2$ values for both
regressions were similar, at approximately .617 and .624 respectively.

\begin{figure}[H]
\includegraphics[width=.4\textwidth]{time_importances.png}
\includegraphics[width=.4\textwidth]{time_graph.png}
\caption{``The feature importances and a plot of the log-transformed viewCount
  predictions against the true viewCounts when using statistics for the 10 most
  recent videos.''}
\includegraphics[width=.4\textwidth]{nlp_importances.png}
\includegraphics[width=.4\textwidth]{nlp_graph.png}
\caption{``The feature importances and a plot of the log-transformed viewCount
  predictions against the true viewCounts when using statistics for the 10 most
  similar videos according to the title's cosine similarity with the candidate title.''}
\end{figure}

\section{Future Work}

Moving forward, we will eliminate features that were found not to be predictive
and add more sophisticated NLP models. We can add extra features to the channel
feature vectors that come from NLP models, or we can use more sophisticated NLP
models to choose the subset of video statistics to include in the feature vector.
It's also possible that using a larger dataset would make the regression perform better.

\begin{thebibliography}{99}

\bibitem{Cd94} Aravind Srinivasan, \emph{YouTuve Views Predictor}, 

    \qquad https://towardsdatascience.com/youtube-views-predictor-9ec573090acb
y\bibitem{Cd94}YouTube Data API: https://developers.google.com/youtube/v3/



\end{thebibliography}

\end{document}
